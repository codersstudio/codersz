llmOptions:
  # The LLM provider to use. For example, 'ollama(gpt-oss:20b)' 'gemini(gemini-2.5-pro)' 'chatgpt(gpt-4o-mini)'
  provider: "ollama"
  # The model to use for LLM requests.
  model: "gpt-oss-safeguard:20b"
  # The host URL for the LLM service.
  url: "http://localhost:11434"
  # The API key for authenticating with the LLM service.
  apiKey: "OLLAMA_API_KEY"
  # Timeout in seconds for LLM requests.
  timeoutSeconds: 300
  stream: true
# Entry point file path for the project
entry: main.jssp
projects:
- # Platform type of the project, e.g., 'dotnet', 'java', 'cpp', 'rust', 'springboot', 'nodejs', 'nodets'
  platform: "cpp"
  # Name of the project
  name: "App"
  # Output path for the project build results
  outPath: "./out/cpp"
  entry: "main.jssp"
- platform: "cppserver"
  name: "App"
  outPath: "./out/cppserver"
  entry: "main.jssp"
- platform: "java"
  name: "App"
  outPath: "./out/java"
  entry: "main.jssp"
  # Project options that affect build and execution settings
  options:
    # The package name for the project.
    package: "com.example.app"
    # The main class name for the Java project.
    mainClass: "App"
- platform: "springboot"
  name: "App"
  outPath: "./out/springboot"
  entry: "main.jssp"
  options:
    package: "com.example.app"
    mainClass: "App"

